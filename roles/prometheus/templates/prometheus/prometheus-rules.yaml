apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: prometheus-rules
data:
  alert-rules.yaml: |
    groups:
    - name: node-cpu-usage
      rules:
      - alert: NodeCPUUsage
        expr: ceil((100 - avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance) * 100)) > 75
        for: 2m
        labels:
          level: Disaster
        annotations:
          summary: "{{$labels.instance}}: High CPU usage detected"
          description: "{{$labels.instance}}: CPU usage is above 75% (current value is: {{ $value }})"
    - name: instance-down
      rules:
      - alert: InstanceDown
        expr: up{job="kubernetes-nodes"} == 0
        for: 1m
        labels:
          level: Disaster
        annotations:
          summary: "Instance {{ $labels.instance }} down"
          description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute."
    - name: node-low-root-disk
      rules:
      - alert: NodeLowRootDisk
        expr: avg by (device,instance)(ceil((container_fs_usage_bytes{id="/",device=~".*/dev.*"}) / container_fs_limit_bytes{id="/",device=~".*/dev.*"} * 100) > 85)
        for: 2m
        labels:
          level: Disaster
        annotations:
          summary: "{{$labels.instance}}: Low disk space"
          description: "{{$labels.instance}}.{{$labels.device}}: disk usage is above 85% (current value is: {{ $value }})"
    - name: node-memory-usage.rules
      rules:
      - alert: NodeMemoryUsage
        expr: avg by (instance) ((node_memory_MemTotal_bytes - node_memory_MemFree_bytes - node_memory_Buffers_bytes - node_memory_Cached_bytes) / node_memory_MemTotal_bytes * 100) > 85
        for: 2m
        labels:
          level: Disaster
        annotations:
          summary: "{{$labels.instance}}: High memory usage detected"
          description: "{{$labels.instance}}: Memory usage is above 85% (current value is: {{ $value }})"
    - name: deployment.rules
      rules:
      - alert: KubeDeploymentUnavailable
        expr: (avg by (deployment,namespace) (kube_deployment_status_replicas_unavailable != 0)) > 0
        for: 5m
        labels:
          level: Disaster
        annotations:
           summary: "{{$labels.namespace}}/{{$labels.deployment}}: unavailable"
           description: "{{$labels.namespace}}/{{$labels.deployment}}: {{ $value}}  replicas unavailable "
    - name: container.rule 
      rules:
      - alert: ContainerRestart
        expr: (avg by (pod,namespace)  (increase(kube_pod_container_status_restarts_total[10m]))) > 3
        for: 5m
        labels:
          level: Disaster
        annotations:
           description: "{{$labels.namespace}}/{{$labels.pod}}: 该 pod 最近 10 分钟内重启次数超过 3 次"
      - alert: ContainerTerminated
        expr: avg by (namespace,pod,reason) (sum_over_time(kube_pod_container_status_terminated_reason{reason!~"(Completed|Error)"}[10m]) == 1)
        for: 5m
        labels:
          level: Disaster
        annotations:
           description: "{{$labels.namespace}}/{{$labels.pod}} stop, the reason is {{$labels.reason}}"
    - name: component.rules
      rules:
      - alert: ComponentDown
        expr: avg by (instance,job) (up) == 0
        for: 5m
        labels:
          level: Disaster
        annotations:
          description: "{{ $labels.job }} has down @all"
    - name: etcd.rules 
      rules:
      - alert: EtcdDown
        expr: avg by (instance,job) (up{job="etcd"}) == 0
        for: 2m
        labels:
          level: Disaster
        annotations:
          description: "{{ $labels.job }} has down @all"
    - name: apiserver.rules 
      rules:
      - alert: ApiserverDown
        expr: absent(up{job="kubernetes-apiservers"} == 1)
        for: 2m
        labels:
          level: Disaster
        annotations:
          description: "K8S api-service has down @all"
    - name: kube-controller-manager.rules 
      rules:
      - alert: K8SControllerManagerDown
        expr: absent(up{job="kube-controller-manager"} == 1)
        for: 2m
        labels:
          level: Disaster
        annotations:
          description: "There is no running K8S controller manager. Deployments and replication controllers are not making progress."
          summary: Controller manager is down @all
    - name: kube-proxy.rules 
      rules:
      - alert: K8SProxyDown
        expr: up{job="kubernetes-kube-proxy"} == 0
        for: 2m
        labels:
          level: Disaster
        annotations:
          description: "Kuber-Proxy is down @all"
    - name: kube-scheduler.rules
      rules:
      - alert: K8SSchedulerDown
        expr: absent(up{job="kubernetes-schedule"} == 1)
        for: 2m
        labels:
          level: Disaster
        annotations:
          description: "There is no running K8S scheduler. New pods are not being assigned
            to nodes."
          summary: Scheduler is down @all
    - name: kubelet.rules
      rules:
      - alert: K8SKubeletDown
        expr: (absent(up{job="kubernetes-kubelet"} == 1) or count(up{job="kubernetes-kubelet"} == 0) / count(up{job="kubernetes-kubelet"}))
          * 100 > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Prometheus failed to scrape {{ $value }}% of kubelets, or all Kubelets have disappeared from service discovery.
          summary: Many Kubelets cannot be scraped
      - alert: K8SNodeNotReady
        expr:  avg by (node) (kube_node_status_condition{condition="Ready",status="true"}) == 0
        for: 10m
        labels:
          level: warning
        annotations:
          description: Prometheus failed to scrape {{ $value }}% of kubelets. 
    - name: node.rules 
      rules:
      - alert: NodeExporterDown
        expr: avg by (instalce,instance) (up{component="node-exporter"}) == 0
        for: 10m
        labels:
          level: Disaster
        annotations:
          description: Prometheus could not scrape a node-exporter for more than 10m,or node-exporters have disappeared from discovery


         
